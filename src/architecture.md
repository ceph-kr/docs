## 확장성과 고가용성

기존 아키텍처에서는 클라이언트가 중앙 집중형 구성 요소(예: 게이트웨이, 브로커, API, 파사드 등)와 통신하며, 이는 복잡한 하위 시스템으로의 단일 진입점 역할을 합니다. 이러한 구조는 성능과 확장성에 제한을 주고, 단일 실패 지점(즉, 중앙 구성 요소가 다운되면 전체 시스템이 다운됨)을 도입하게 됩니다.

Ceph는 중앙 집중형 게이트웨이를 제거하여 클라이언트가 Ceph OSD 데몬과 직접 상호 작용할 수 있도록 하여 이러한 한계를 극복합니다. Ceph OSD 데몬은 다른 Ceph 노드에 오브젝트 복제본을 생성하여 데이터 안전성과 고가용성을 보장합니다. 또한 Ceph는 모니터 클러스터를 사용하여 고가용성을 보장합니다. 중앙 집중화를 제거하기 위해 Ceph는 **CRUSH**라는 알고리즘을 사용합니다.

## CRUSH 소개

Ceph 클라이언트와 Ceph OSD 데몬은 모두 **CRUSH** 알고리즘을 사용하여 오브젝트 위치에 대한 정보를 효율적으로 계산하며, 중앙 조회 테이블에 의존하지 않습니다. **CRUSH**는 이전 접근 방식보다 더 나은 데이터 관리 메커니즘을 제공하며, 클러스터의 모든 클라이언트와 OSD 데몬에게 작업을 고르게 분산시켜 대규모 확장을 가능하게 합니다. **CRUSH**는 지능적인 데이터 복제를 통해 복원력을 보장하며, 이는 초대규모 스토리지에 더 적합합니다. 아래는 CRUSH 작동 방식에 대한 세부 정보입니다. 자세한 논의는 [CRUSH - 분산된 복제 데이터의 제어 가능하고 확장 가능한 배치](https://docs.ceph.com/en/latest/rados/operations/crush-map/)를 참고하세요.

## 클러스터 맵

Ceph는 클라이언트와 Ceph OSD 데몬이 클러스터 토폴로지에 대한 정보를 공유하는 다섯 가지 맵을 기반으로 합니다. 이 맵들을 총칭하여 “클러스터 맵”이라고 합니다:

1. **모니터 맵 (Monitor Map)**: 클러스터 fsid, 각 모니터의 위치, 이름, 주소 정보를 포함합니다. 또한 현재 에폭(epoch), 맵 생성 시점, 마지막 수정 시점을 포함합니다. 모니터 맵을 확인하려면 다음 명령어를 실행하세요:
